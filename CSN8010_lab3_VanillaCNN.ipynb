{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d762e59b",
   "metadata": {},
   "source": [
    "# **CSN8010 Practical Lab 3 Vanilla CNN and Fine-Tune VGG16 - for Dogs and Cats Classification**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d623c3f",
   "metadata": {},
   "source": [
    "**Problem Framing**\n",
    "\n",
    "The aim of this lab is to predict the **cat or dog** class of an image using a **Vanilla CNN** and a **Fine-Tune VGG16** model.\n",
    "\n",
    "To compare the performance of both models, I will use the following metrics:\n",
    "- **Accuracy**: The proportion of correct predictions made by the model.\n",
    "- **Precision**: The proportion of true positive predictions made by the model out of all positive predictions.\n",
    "- **Recall**: The proportion of true positive predictions made by the model out of all actual\n",
    "positive instances in the dataset.\n",
    "- **F1 Score**: The harmonic mean of precision and recall, providing a balance between the two metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf7cc3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m paths\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'imutils'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\n",
    "import numpy as np\n",
    "import os, pathlib\n",
    "from imutils import paths\n",
    "import random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20d3d67",
   "metadata": {},
   "source": [
    "## **1. Get the data**\n",
    "\n",
    "The original dataset was downloaded from Kaggle, but this contained `25,000` thousand of photos. To create a smaller subset of the dataset, I executed the following Python script:\n",
    "\n",
    "```python\n",
    "import os, shutil, pathlib\n",
    "\n",
    "original_dir = pathlib.Path(\"../data/kaggle_dogs_vs_cats/train\")\n",
    "new_base_dir = pathlib.Path(\"../data/kaggle_dogs_vs_cats_small\")\n",
    "\n",
    "def make_subset(subset_name, start_index, end_index):\n",
    "    for category in (\"cat\", \"dog\"):\n",
    "        dir = new_base_dir / subset_name / category\n",
    "        os.makedirs(dir)\n",
    "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
    "        for fname in fnames:\n",
    "            shutil.copyfile(src=original_dir / fname,\n",
    "                            dst=dir / fname)\n",
    "\n",
    "make_subset(\"train\", start_index=0, end_index=1000)\n",
    "make_subset(\"validation\", start_index=1000, end_index=1500)\n",
    "make_subset(\"test\", start_index=1500, end_index=2500)\n",
    "```\n",
    "\n",
    "Te new data set contains:\n",
    "\n",
    "- 1000 training images.\n",
    "- 500 validation images.\n",
    "- 1000 test images per class\n",
    "\n",
    "> This reduces the dataset from 25,000 images to `5,000`. Additionally, I split the dataset into three subsets train, validation, and test. The training set contains `1000` images per class, the validation set contains `500` images per class, and the test set contains `1000` images per class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b7c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_base_dir = pathlib.Path(\"./data/kaggle_dogs_vs_cats_small\")\n",
    "\n",
    "# Count the number of files in the new base directory\n",
    "for subset in [\"train\", \"validation\", \"test\"]:\n",
    "    num_files = len(list(paths.list_images(new_base_dir/subset)))\n",
    "    print(f\"Number of files in {subset}: {num_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2771f619",
   "metadata": {},
   "source": [
    "## **2. Data Exploration and Preprocessing**\n",
    "\n",
    "In this section, I will explore the dataset to understand its structure and content.\n",
    "\n",
    "**Show some random images from the dataset.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_images(base_path, subset, category, n=5):\n",
    "    image_dir = Path(base_path) / subset / category\n",
    "    images = list(image_dir.glob(\"*.jpg\"))\n",
    "    random_images = random.sample(images, n)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for i, image_path in enumerate(random_images):\n",
    "        img = Image.open(image_path)\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(image_path.name)\n",
    "    plt.suptitle(f\"Random {category} images from {subset} set\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_random_images(new_base_dir, 'train', 'cat')\n",
    "show_random_images(new_base_dir, 'train', 'dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a424d2",
   "metadata": {},
   "source": [
    "**Validate the size of the images in the training set.**\n",
    "\n",
    "It is important to ensure that all images have the same size, as this is a requirement for training a CNN model. I will check the size of the images in the training set and print the frequency of each size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf1945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = []\n",
    "modes = []\n",
    "\n",
    "print(f\"VALIDATING BALANCE OF DATASET...\")\n",
    "\n",
    "for subset in [\"train\", \"validation\", \"test\"]:\n",
    "    print(f\"\\n--- {subset.upper()} ---\")\n",
    "    for category in [\"cat\", \"dog\"]:       \n",
    "        count = len(list((new_base_dir / subset / category).glob(\"*.jpg\")))\n",
    "        print(f\"{category}: {count}\") \n",
    "        for path in (new_base_dir / subset / category).glob(\"*.jpg\"):\n",
    "            img = Image.open(path)\n",
    "            sizes.append(img.size)\n",
    "            modes.append(img.mode)  # 'RGB', 'L', 'RGBA', etc.\n",
    "size_counts = Counter(sizes)\n",
    "mode_counts = Counter(modes)\n",
    "\n",
    "print(f\"\\nVALIDATING IMAGE SIZES (10 MOST COMMON)...\")\n",
    "for size, count in size_counts.most_common(10):\n",
    "    print(f\"{size}: {count} images\")\n",
    "\n",
    "print(f\"\\nVALIDATING IMAGE MODES...\")\n",
    "for mode, count in Counter(modes).most_common():\n",
    "    print(f\"{mode}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89f3c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "widths = [w for w, h in sizes]\n",
    "heights = [h for w, h in sizes]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(widths, bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Distribution of widths\")\n",
    "plt.xlabel(\"widths (pixels)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(heights, bins=30, color='salmon', edgecolor='black')\n",
    "plt.title(\"Distribution of heights\")\n",
    "plt.xlabel(\"heights (pixels)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0552352",
   "metadata": {},
   "source": [
    "> In the exploration I found:\n",
    "\n",
    "    - There are some images with different sizes\n",
    "    - There are no images with different modes (all are RGB)\n",
    "    - Balance between classes (cats and dogs) is maintained in the training, validation, and test sets.\n",
    "    \n",
    "I will resize them to a common size of `150x150` pixels. This is a common practice in image classification tasks to ensure that all images have the same dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a06abf6",
   "metadata": {},
   "source": [
    "### **Preprocessing the images**\n",
    "\n",
    "In this section, I will preprocess the images to ensure they are in the correct format for training a CNN model. This includes resizing the images to a common size of `150x150` pixels, normalizing the pixel values to be between `0` and `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c41e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mapping class name to numeric label\n",
    "class_labels = {\"cat\": 0, \"dog\": 1}\n",
    "\n",
    "def load_images_from_folder(folder_path, target_size=(150, 150)):\n",
    "\n",
    "    x = []\n",
    "    labels = []\n",
    "\n",
    "    image_paths = list(paths.list_images(folder_path))\n",
    "    random.shuffle(image_paths)\n",
    "\n",
    "\n",
    "    for image_path, i in zip(image_paths, range(len(image_paths))):\n",
    "        print(f\"Loading image... {i} / {len(image_paths)}\", end=\"\\r\")\n",
    "        try:\n",
    "            img = Image.open(image_path).resize(target_size) # Resize image to target size\n",
    "            img = np.array(img).astype(\"float32\") / 255.0  # Normalize to [0, 1]\n",
    "            x.append(img)\n",
    "\n",
    "            # Determine label from filename\n",
    "            file_name = os.path.basename(image_path)\n",
    "            if file_name.startswith(\"cat\"):\n",
    "                labels.append(class_labels[\"cat\"])\n",
    "            elif file_name.startswith(\"dog\"):\n",
    "                labels.append(class_labels[\"dog\"])\n",
    "            else:\n",
    "                print(f\"Warning: {file_name} does not match known class... Skipping...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "\n",
    "    x = np.array(x)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    print(f\"✅ Loaded {len(x)} images from {folder_path.name}. Shape: {x.shape}\")\n",
    "    return x, labels\n",
    "\n",
    "# Example usage:\n",
    "x_train, y_train = load_images_from_folder(new_base_dir / \"train\", target_size=(150, 150))\n",
    "x_val, y_val     = load_images_from_folder(new_base_dir / \"validation\", target_size=(150, 150))\n",
    "x_test, y_test   = load_images_from_folder(new_base_dir / \"test\", target_size=(150, 150))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442eda1e",
   "metadata": {},
   "source": [
    "**Showing the first image in the training set to verify that the preprocessing was successful.**\n",
    "\n",
    "This image is part of a 4D array with shape: `(batch_size, height, width, channels)`.\n",
    "\n",
    "When I access x_train[0], I am retrieving one image with shape (height, width, 3), represented as a 3D NumPy array. All data is normalized to the range [0, 1].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ecab91",
   "metadata": {},
   "source": [
    "**Showing one image and its label from the training set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796f7d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(x, label_batch, class_labels=class_labels, index=6):\n",
    "\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    class_names = list(class_labels.keys())  # Get class\n",
    "    idxs = np.random.choice(len(x), size=index, replace=False)\n",
    "\n",
    "    for i, idx in enumerate(idxs):\n",
    "        ax = plt.subplot(2, 3, i + 1)  # Arrange in 2*3 grid\n",
    "        plt.imshow(x[idx])  \n",
    "        plt.title(class_names[label_batch[idx]].title())  # Display the label\n",
    "        plt.axis('off')  # Hide axis ticks\n",
    "    plt.tight_layout()               \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_image(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343a4290",
   "metadata": {},
   "source": [
    "## **3. Architecture Design**\n",
    "\n",
    "In this section I will design the architecture of the CNN model. \n",
    "\n",
    "### **CNN Sequential Model**\n",
    "\n",
    "Layers:\n",
    "\n",
    "1. Conv2D \n",
    "2. MaxPooling2D (32 filters, 3x3 kernel, ReLU activation)\n",
    "3. Conv2D\n",
    "4. MaxPooling2D (64 filters, 3x3 kernel, ReLU activation)\n",
    "5. Flatten\n",
    "6. Dense (hiddden neurons) \n",
    "7. Dropout (0.5)\n",
    "8. Final Dense with sigmoid (binary output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7df30c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential() # Create a Sequential model, piled layer by layer\n",
    "\n",
    "# FIRST LAYER: Conv2D (32) \n",
    "model.add(layers.Conv2D(\n",
    "    filters=32,               # filter or feature detection size\n",
    "    kernel_size=(3, 3),       # size of each filter (3x3 pixels)\n",
    "    activation='relu',        # Relu activation Function to introduce non-linearity \n",
    "    input_shape=(150, 150, 3) # input shape: 150*150 pixels with 3 channels (RGB)\n",
    "))\n",
    "\n",
    "# SECOND LAYER: MaxPooling2D (reducing image size)\n",
    "model.add(layers.MaxPooling2D(\n",
    "    pool_size=(2, 2) # Only keep the maximum value in each 2x2 region\n",
    "))\n",
    "\n",
    "# THIRD LAYER: Conv2D (64) \n",
    "model.add(layers.Conv2D(\n",
    "    filters=64,       # Increasing the number of filters to learn more complex features        \n",
    "    kernel_size=(3, 3),       \n",
    "    activation='relu',        \n",
    "))\n",
    "\n",
    "# FOURTH LAYER: MaxPooling2D (reducing image size)\n",
    "model.add(layers.MaxPooling2D(\n",
    "    pool_size=(2, 2) # Only keep the maximum value in each 2x2 region\n",
    "))\n",
    "\n",
    "# FIFTH LAYER: FLATTEN TO CONVERT 3D TO 1D\n",
    "model.add(layers.Flatten()) # Output 1D to feed into Dense layers\n",
    "\n",
    "# SIXTH LAYER: Dense (128) Fully Connected + Dropout (0.5)\n",
    "model.add(layers.Dense(\n",
    "    units=64,         # Number of neurons in this layer\n",
    "    activation='relu' \n",
    "))\n",
    "\n",
    "# SEVENTH LAYER: Dropout (0.6)\n",
    "model.add(layers.Dropout(rate=0.6)) # Dropout to prevent overfitting\n",
    "\n",
    "# FINAL LAYER: Dense (1) Output Layer\n",
    "model.add(layers.Dense(\n",
    "    units=1,            # Single neuron for binary output (0 or 1)\n",
    "    activation='sigmoid' # Outputs a probability between 0 and 1\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da47e84a",
   "metadata": {},
   "source": [
    "**Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6632b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126a9643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with appropriate loss function and optimizer\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b8781d",
   "metadata": {},
   "source": [
    "#### **Setting the Callbacks and EarlyStopping**\n",
    "\n",
    "In this part of the lab, I set up callbacks to monitor the training process and save the best model based on validation accuracy.  \n",
    "I also used the `EarlyStopping` function to automatically stop the training when the model stopped improving, based on the `patience` parameter.  \n",
    "This helps prevent overfitting and ensures that the best-performing version of the model is retained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825aecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback to save the best version of the model (based on validation accuracy)\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath='best_model.h5',        # File where the best model will be saved\n",
    "    monitor='val_accuracy',          # Metric to monitor\n",
    "    save_best_only=True,             # Only save the model if it's the best so far\n",
    "    mode='max',                      # We want to maximize validation accuracy\n",
    "    verbose=1                        # Print a message each time the model is saved\n",
    ")\n",
    "\n",
    "\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=2,              # Stop after 5 epochs without improvement\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c1c1cc",
   "metadata": {},
   "source": [
    "#### ***Training the model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5863c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using training data, validate on validation data\n",
    "history = model.fit(\n",
    "    x_train, y_train,               # Training data and labels\n",
    "    epochs=20,                      # Number of times the model will see the full dataset\n",
    "    batch_size=32,                 # Number of samples per gradient update\n",
    "    validation_data=(x_val, y_val),# Validation data to evaluate on after each epoch\n",
    "    callbacks=[checkpoint_cb, earlystop_cb]  # Save best model + stop if no improvement\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e266312",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Accuracy plot\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f1625d",
   "metadata": {},
   "source": [
    "> *Those graphs will show the training and validation accuracy and loss over the epochs.*\n",
    "- The accuracy graph shows that the training accuracy increases while the validation accuracy decreases, indicating overfitting. \n",
    "- In the loss graph, we can see that the training loss decreases while the validation loss increases, which is also a sign of overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75c4071",
   "metadata": {},
   "source": [
    "\n",
    "### **Fine- Tune VGG16**\n",
    "\n",
    "In this step, I will use a pre-trained VGG16 model as a base and fine-tune it for the dog vs cat classification task. The VGG16 model is a well-known convolutional neural network architecture that has been pre-trained on the ImageNet dataset.\n",
    "\n",
    "Then, I going to add my own layers on the top of the VGG16 model to adapt it to the dog vs cat classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52e8709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG16 base model without the top classifier layers\n",
    "vgg_base = VGG16(\n",
    "    weights='imagenet',        # Load pre-trained weights\n",
    "    include_top=False,         \n",
    "    input_shape=(150, 150, 3)  # Match the shape of your input images\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1add98",
   "metadata": {},
   "source": [
    "> To do not change the weights of the VGG16 model, I will freeze the layers of the base model. This means that the weights of the VGG16 model will not be updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0066284",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_base.trainable = False  # Freeze all convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deae5c0",
   "metadata": {},
   "source": [
    "**Creating my top layers**\n",
    "\n",
    "1. Flatten\n",
    "2. Dense (hidden neurons)\n",
    "3. Dropout (0.5)\n",
    "4. Final Dense with sigmoid (binary output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e458ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model on top of the frozen VGG16 base\n",
    "model_vgg = models.Sequential()\n",
    "\n",
    "# Add the VGG16 convolutional base\n",
    "model_vgg.add(vgg_base)\n",
    "\n",
    "# Add custom classifier on top\n",
    "\n",
    "# FIRST LAYER: Flatten the output of the conv base\n",
    "model_vgg.add(layers.Flatten())            \n",
    "\n",
    "# SECOND LAYER: Fully connected layer\n",
    "model_vgg.add(layers.Dense(64, activation='relu'))     \n",
    "\n",
    "# THIRD LAYER: Dropout for regularization\n",
    "model_vgg.add(layers.Dropout(0.5))  \n",
    "\n",
    "#FOURTH LAYER: Final Dense layer for binary classification (Output layer)\n",
    "model_vgg.add(layers.Dense(1, activation='sigmoid'))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc26701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with appropriate loss function and optimizer\n",
    "model_vgg.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b887e518",
   "metadata": {},
   "source": [
    "#### **Setting the callbacks and earlystop for VGG16**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ffe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath='best_vgg_model.h5',     # File to save best version of VGG16 model\n",
    "    monitor='val_accuracy',           # Watch validation accuracy\n",
    "    save_best_only=True,              # Save only the best model\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor='val_accuracy',          # Stop if val accuracy stops improving\n",
    "    patience=3,                      # Wait 3 epochs before stopping\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf131232",
   "metadata": {},
   "source": [
    "#### ***Training the model with VGG16***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53533fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using training data, validate on validation data\n",
    "history_vgg = model_vgg.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[checkpoint_cb, earlystop_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd403d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Accuracy plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_vgg.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history_vgg.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_vgg.history['loss'], label='Train Loss')\n",
    "plt.plot(history_vgg.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0383576b",
   "metadata": {},
   "source": [
    "> The accuracy graph shows that while training accuracy steadily increases, validation accuracy begins to fluctuate after a few epochs. The EarlyStopping function effectively halted training to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3a2d92",
   "metadata": {},
   "source": [
    "## **4. Performing the models**\n",
    "\n",
    "Loading the best version of each model. To automate this, I going to create a function and then evaluate both conventional and using VGG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e4b0f3",
   "metadata": {},
   "source": [
    "### **Creating a function to evaluate models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccda269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(name, model_path):\n",
    "    best_model = load_model(model_path)\n",
    "    # Predictions\n",
    "    y_probs = best_model.predict(x_test)\n",
    "    y_preds = (y_probs > 0.5).astype(\"int32\") # Convert probs to class\n",
    "    \n",
    "    # Accuracy and loss\n",
    "    test_loss, test_accuracy = best_model.evaluate(x_test, y_test)\n",
    "    print(f\"\\n🔍 Evaluation for {name}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Confusion Matrix - Positives and False\n",
    "    cm = confusion_matrix(y_test, y_preds)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix – {name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "    #classification_report - precision, recall y F1-score\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_preds, target_names=[\"Cat\", \"Dog\"]))\n",
    "\n",
    "\n",
    "    # precision_recall_curve- Variation\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "    \n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(recalls, precisions, marker='.')\n",
    "    plt.title(f\"Precision–Recall Curve – {name}\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e696c1",
   "metadata": {},
   "source": [
    "### **Test Set Evaluation – Custom Convolutional Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_models('Custom CNN','best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6c506a",
   "metadata": {},
   "source": [
    "> The accuracy in the test set is `0.67`, which indicates that the model is not performing well on unseen data. This is likely due to overfitting, as the model performs well on the training and validation sets but fails to generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5b1a40",
   "metadata": {},
   "source": [
    "### **Test Set Evaluation – VGG16 Fine-Tuned Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec628dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_models(\"VGG16 Fine-Tuned\", \"best_vgg_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6baffaa",
   "metadata": {},
   "source": [
    "> The accuracy with VGG16 reached `0.89`, which shows a clear improvement compared to the previous model's accuracy of `0.67`. Its validation performance was more stable, and the use of **EarlyStopping** helped avoid overfitting. Overall, the model generalizes well and is expected to perform effectively on new, unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff566e5",
   "metadata": {},
   "source": [
    "### **Conclusions - Comparing models**\n",
    "\n",
    "\n",
    "The `VGG16 Fine-Tuned` maintained high precision even at high levels of recall. In contrast, the custom CNN model showed an imbalance between precision and recall. The `F1 Score` was significantly better for the VGG16 Fine-Tuned, reaching **89%** for dogs and **90%** for cats. A visual summary of this comparison is provided in the next image:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/Model_Comparison.png\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55a987",
   "metadata": {},
   "source": [
    "### **Showing Misclassified Images**\n",
    "\n",
    "To show some examples from the test set that were misclassified for each model. I going to create a function too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ea255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_misclassified_images(model_path, name, num_images=9):\n",
    "    model = load_model(model_path)\n",
    "    y_probs = model.predict(x_test)\n",
    "    y_preds = (y_probs > 0.5).astype(\"int32\")\n",
    "\n",
    "    misclassified_idxs = np.where(y_preds.flatten() != y_test)[0]\n",
    "\n",
    "    if len(misclassified_idxs) == 0:\n",
    "        print(f\"No misclassified images found for {name}.\")\n",
    "        return\n",
    "\n",
    "    sample_idxs = np.random.choice(misclassified_idxs, size=min(num_images, len(misclassified_idxs)), replace=False)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, idx in enumerate(sample_idxs):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(x_test[idx])\n",
    "        true_label = \"Dog\" if y_test[idx] == 1 else \"Cat\"\n",
    "        pred_label = \"Dog\" if y_preds[idx] == 1 else \"Cat\"\n",
    "        plt.title(f\"True: {true_label}, Pred: {pred_label}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.suptitle(f\"Misclassified Examples – {name}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395a0001",
   "metadata": {},
   "source": [
    "_The images below show the true label vs. the predicted label for each failure case._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c07def",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_misclassified_images(\"best_model.h5\", \"Custom CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307b7f6a",
   "metadata": {},
   "source": [
    "> The `Custom CNN` model reveals specific types of images where it tends to struggle:\n",
    "\n",
    "- Images containing **more than one pet**\n",
    "- **Black and white dogs**, which are sometimes confused with cats\n",
    "- **Blurry backgrounds** that make the subject harder to identify\n",
    "- **Unusual angles or rotations**, as well as **non-standard backgrounds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c5aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_misclassified_images(\"best_vgg_model.h5\", \"VGG16 Fine-Tuned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d5a22b",
   "metadata": {},
   "source": [
    "> The `VGG16 Fine-Tuned` model misclassified some:\n",
    "\n",
    "- **dogs with unusual poses**\n",
    "- As well as **small pets** or **pets with objects partially covering their face** (e.g., toys, blankets, or hands). \n",
    "- Pictures with letters.\n",
    "\n",
    "These situations may obscure key visual features used by the model to differentiate classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aa3778",
   "metadata": {},
   "source": [
    "## **5. Final Conclusions**\n",
    "\n",
    "As demonstrated in the previous sections, and after comparing both models, the `VGG16 Fine-Tuned` model clearly outperformed the custom CNN across all key evaluation metrics: **accuracy, precision, recall, F1-score**, and overall stability on the test set.\n",
    "\n",
    "In addition, it made fewer classification errors, as shown in the confusion matrix and visual analysis of misclassified examples.\n",
    "\n",
    "Although the VGG16-based model takes **more than 10 minutes to train**, the improvement in accuracy and generalization makes the trade-off worthwhile.\n",
    "\n",
    "**Therefore, the VGG16 Fine-Tuned model is more suitable for this binary image classification task.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
